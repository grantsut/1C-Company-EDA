{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;font-size:xx-large;font-weight:bold\"> Exploratory data analysis example - 1C Company</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows some exploratory data exploration of retail sales data in the context of predicting future sales numbers of items at different retail outlets.  \n",
    "\n",
    "This data was provided by the Russian software publisher and retailer 1C Company, for a Kaggle competition in which the challenge is to predict monthly sales for specific products in specific shops. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "  \n",
    "* [Load, check and clean data](#Load,-check-and-clean-data)\n",
    "* [Exploration of trends and distributions in the data](#Exploration-of-trends-and-distributions-in-the-data)\n",
    "* [Model fitting and fitted model exploration](#Model-fitting-and-fitted-model-exploration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031398,
     "end_time": "2021-05-06T17:35:42.342148",
     "exception": false,
     "start_time": "2021-05-06T17:35:42.310750",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load, check and clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import python packages\n",
    "The first thing to do is import a standard set of Python packages for data manipulation and analysis. Pandas and NumPy are used to store and manipulate tabular data, Matplotlib and Seaborn are used for visualization, and a few standard Python libraries are imported also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 0.996567,
     "end_time": "2021-05-06T17:35:43.437658",
     "exception": false,
     "start_time": "2021-05-06T17:35:42.441091",
     "status": "completed"
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"seaborn\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1.0, rc={\"axes.titlesize\":13})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.073986,
     "end_time": "2021-05-12T11:10:18.590794",
     "exception": false,
     "start_time": "2021-05-12T11:10:18.516808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, silent=True, allow_categorical=True, float_dtype=\"float32\"):\n",
    "    \"\"\"\n",
    "    Iterates through all the columns of a dataframe and downcasts the data type\n",
    "     to reduce memory usage. Can also factorize categorical columns to integer dtype.\n",
    "    \"\"\"\n",
    "\n",
    "    def _downcast_numeric(series, allow_categorical=allow_categorical):\n",
    "        \"\"\"\n",
    "        Downcast a numeric series into either the smallest possible int dtype or a specified float dtype.\n",
    "        \"\"\"\n",
    "        if pd.api.types.is_sparse(series.dtype) is True:\n",
    "            return series\n",
    "        elif pd.api.types.is_numeric_dtype(series.dtype) is False:\n",
    "            if pd.api.types.is_datetime64_any_dtype(series.dtype):\n",
    "                return series\n",
    "            else:\n",
    "                if allow_categorical:\n",
    "                    return series\n",
    "                else:\n",
    "                    codes, uniques = series.factorize()\n",
    "                    series = pd.Series(data=codes, index=series.index)\n",
    "                    series = _downcast_numeric(series)\n",
    "                    return series\n",
    "        else:\n",
    "            series = pd.to_numeric(series, downcast=\"integer\")\n",
    "        if pd.api.types.is_float_dtype(series.dtype):\n",
    "            series = series.astype(float_dtype)\n",
    "        return series\n",
    "\n",
    "    if silent is False:\n",
    "        start_mem = np.sum(df.memory_usage()) / 1024 ** 2\n",
    "        print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    if df.ndim == 1:\n",
    "        df = _downcast_numeric(df)\n",
    "    else:\n",
    "        for col in df.columns:\n",
    "            df.loc[:, col] = _downcast_numeric(df.loc[:, col])\n",
    "    if silent is False:\n",
    "        end_mem = np.sum(df.memory_usage()) / 1024 ** 2\n",
    "        print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "        print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll list the files in the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'items.csv',\n",
       " 'item_categories.csv',\n",
       " 'sales_train.csv',\n",
       " 'sample_submission.csv',\n",
       " 'shops.csv',\n",
       " 'test.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR_PATH = \"../input/competitive-data-science-predict-future-sales/\"\n",
    "os.listdir(DIR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load all the files into the workspace as pandas dataframes with the same variable names as the csv files, other than \"sales_train\", which will be renamed to \"train\" to save time typing. Additionally, two Russian language text columns are replaced with English translated versions>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_221064/2971730370.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"supercategory\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"platform\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"digital\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"supercategory_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"platform_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../input/competitive-data-science-predict-future-sales/sales_train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m# shops = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m shops = pd.read_csv(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\eda\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\eda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\eda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\eda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\eda\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m                 \u001b[1;31m# destructive to chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\eda\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\eda\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\eda\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\eda\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "items = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\n",
    "item_categories = pd.read_csv(\n",
    "    \"../input/predict-future-sales-extra/item_categories_enhanced.csv\"\n",
    ")\n",
    "item_categories = item_categories.drop(\n",
    "    columns=[\"supercategory\", \"platform\", \"digital\", \"supercategory_id\", \"platform_id\"]\n",
    ")\n",
    "train = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\n",
    "shops = pd.read_csv(\"../input/predict-future-sales-extra/shops_extra.csv\")\n",
    "shops = shops.drop(columns=\"shop_name\").rename(columns={\"shop_name_en\": \"shop_name\"})\n",
    "test = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032732,
     "end_time": "2021-05-06T17:35:49.048141",
     "exception": false,
     "start_time": "2021-05-06T17:35:49.015409",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Show table contents\n",
    "We have a quick look at first few lines of each of the tables to get a basic idea of their contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.063351,
     "end_time": "2021-05-06T17:35:49.145980",
     "exception": false,
     "start_time": "2021-05-06T17:35:49.082629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_categories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge tables  \n",
    "It looks like the training data has been structured as normalized tables for efficiency. For convenience we can merge the training data into a single table by joining them on their shared fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(items, on='item_id', how='left')\n",
    "train = train.merge(item_categories, on='item_category_id', how='left')\n",
    "train = train.merge(shops, on='shop_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check and convert datatypes\n",
    "We list the datatypes of the columns to check if they could be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date field is formatted as a string, we can convert that to the datetime dtype to enable extra datetime features such as grouping by weeks or months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"date\"] = pd.to_datetime(train[\"date\"], format=\"%d.%m.%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other columns have an appropriate datatype but could be downcasted to a lower-precision datatype to save memory without any loss in accuracy. This is done here with a custom downcasting function that downcasts integer columns to the smallest type that can represent all column values, and converts float-type columns to the float32 type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = reduce_mem_usage(train)\n",
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for missing values\n",
    "Now we have prepared the dataframe we can check it for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't any, good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First look and outlier removal\n",
    "The full dataset is assembled, so we can look at the first few rows to get an idea of its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the rows are individual sales counts for specific shop-item combinations on specific days, with the following columns:\n",
    "\n",
    "* date: the date of the transaction\n",
    "* date_block_num: the month of the transaction encoded as an integer\n",
    "* shop_id: the shop which performed the sales, encoded as an integer\n",
    "* item_id: the item which was sold or returned, encoded as an integer\n",
    "* item_price: the price that the item was traded at in the shop on that day\n",
    "* item_cnt_day: the net number of items that were sold in the shop that day (zero entries are omitted)\n",
    "* item_name: a text string with a natural language description of the item, in Russian or English\n",
    "* item_category_id: the category of the item, encoded as an integer\n",
    "* item_category_name: the category of the item in English\n",
    "* shop_name: an English translation of the name of the shop\n",
    "\n",
    "We now use the pandas \"describe\" method to get a summary of the distribution of values in each numerical column. We'll round values to the nearest integer for clarity and append the number of unique values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        train.select_dtypes('number').describe().astype(\"int\"),\n",
    "        pd.DataFrame(train.select_dtypes('number').nunique(), columns=[\"nunique\"]).transpose(),\n",
    "    ],\n",
    "    axis=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see:\n",
    "* date_block_num, shop_id and item_category_id are categorical variables numbered sequentially from zero with respectively 34, 60 and 84 unique values.\n",
    "* item_id is a categorical variable with over 20,000 unique values.\n",
    "* item_price has at least one negative value, which could be an error or a missing value marker.\n",
    "  * Some quick investigation finds that only one entry has a negative price, which should be safe to remove.\n",
    "* item_cnt_day also has one or more negative values, which could indicate item returns.\n",
    "  * There are 7356 rows with negative item_cnt_day values, 0.25% of the total. These are probably valid, but it turns out that the negative values cause problems when aggregating sales (e.g. negative monthly sales figures), so we delete these here.\n",
    "* item_price and item_cnt_day both contain max values much higher than their 75% percentile values, which indicates the presence of outliers or long-tailed distributions.\n",
    "\n",
    "The very highest-valued entries of the item_price and item_cnt_month are so few in number they can be looked at individually. Judging by a quick inspection, the very highest valued entries are custom orders for large numbers of items and are probably best removed.\n",
    "\n",
    "We remove outliers and negative values by retaining only rows with item_price and item_cnt_day values within specific thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train[(train[\"item_price\"] > 0) & (train[\"item_price\"] < 50000)]\n",
    "train = train[(train[\"item_cnt_day\"] > 0) & (train[\"item_cnt_day\"] < 1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.045998,
     "end_time": "2021-05-06T17:36:49.565890",
     "exception": false,
     "start_time": "2021-05-06T17:36:49.519892",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Checking the validity of shop_id and shop_name column values\n",
    "The training data contains multiple shop_ids, not all of which are in the test set. To get an overview we can create a figure which plots total sales for each shop by month and displays the shop names in Russian and English. A table with a list of translated shop names is used for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 9.034654,
     "end_time": "2021-05-06T17:36:58.649362",
     "exception": false,
     "start_time": "2021-05-06T17:36:49.614708",
     "status": "completed"
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "def sales_by_shop():\n",
    "    shops = pd.read_csv(\"../input/predict-future-sales-extra/shops_extra.csv\")\n",
    "    shops[\"shop_name_trans\"] = shops[\"shop_name\"] + \" \\n\" + shops[\"shop_name_en\"]\n",
    "    shopnamedict = {\n",
    "        id: f\"{id:02}: {name}\"\n",
    "        for name, id in zip(list(shops[\"shop_name_trans\"]), list(shops[\"shop_id\"]))\n",
    "    }\n",
    "\n",
    "    ts = train.groupby([\"date_block_num\", \"shop_id\"])[\"item_cnt_day\"].sum()\n",
    "    ts = ts.reset_index()\n",
    "    ts[\"shop_name_trans\"] = ts[\"shop_id\"].map(shopnamedict)\n",
    "    tsp = ts.pivot(index=\"date_block_num\", columns=\"shop_name_trans\", values=\"item_cnt_day\")\n",
    "    fig, axes = plt.subplots(20, 3, figsize=(15, 30))\n",
    "    for i, shop_name in enumerate(tsp.columns):\n",
    "        idx = divmod(i, axes.shape[1])\n",
    "        axes[idx[0]][idx[1]].plot(tsp.index, tsp.loc[:, shop_name], marker=\".\")\n",
    "        axes[idx[0]][idx[1]].set_xlim(0, 34)\n",
    "        axes[idx[0]][idx[1]].set_title(shop_name, fontsize=10)\n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "sales_by_shop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Correct shop labels and remove data from unusual shops\n",
    "A closer look at these plots finds several data cleaning issues, such as:\n",
    "* Duplicate shops with minor name differences. Duplicates should be merged.\n",
    "* Infrequently-used special category shops such as \"Outbound Trade\". These might distort the model if used during training.\n",
    "* Some shops cease to exist before the end of the time period covered.\n",
    "\n",
    "We will merge the duplicate shops and remove all data from shops not in the test month, for simplicity.  \n",
    "\n",
    "(A similar check of item categories finds no data quality issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"shop_id\"] = train[\"shop_id\"].replace({0: 57, 1: 58, 11: 10, 40: 39})\n",
    "train = train.loc[train.shop_id.isin(test[\"shop_id\"].unique()), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicate items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll check for remaining duplicate entries in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 5 duplicate entries, but the fact that 4 of them are for the same product suggests that they are errors, so we might as well drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of trends and distributions in the data\n",
    "\n",
    "We can look at the the distributions of target and other features, and look for interesting relationships between features. The aim of this exploration should be to find patterns in the data that could help predict the target value, and identify the types of model that are appropriate for the prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model training dataset by aggregating the data by month\n",
    "The challenge is to predict monthly sales totals, so we should create a model training set that reproduces the test format by summing sales for each month.   \n",
    "\n",
    "The format of the test items is a list of all possible combinations of shops and items for shops and items that recorded at least one sale in the test month, i.e. the Cartesian product of these shops and items. We recreate this by summing the sales for the Cartesian product of active shops and items sold in each month.\n",
    "\n",
    "As before, we merge the items, categories and shops tables so these can be used as predictive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.06913,
     "end_time": "2021-05-12T11:10:23.879060",
     "exception": false,
     "start_time": "2021-05-12T11:10:23.809930",
     "status": "completed"
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "def create_testlike_train(sales_train, test=None):\n",
    "    indexlist = []\n",
    "    for i in sales_train.date_block_num.unique():\n",
    "        x = itertools.product(\n",
    "            [i],\n",
    "            sales_train.loc[sales_train.date_block_num == i].shop_id.unique(),\n",
    "            sales_train.loc[sales_train.date_block_num == i].item_id.unique(),\n",
    "        )\n",
    "        indexlist.append(np.array(list(x)))\n",
    "    df = pd.DataFrame(\n",
    "        data=np.concatenate(indexlist, axis=0),\n",
    "        columns=[\"date_block_num\", \"shop_id\", \"item_id\"],\n",
    "    )\n",
    "\n",
    "    # Add revenue column to sales_train\n",
    "    sales_train[\"item_revenue_day\"] = sales_train[\"item_price\"] * sales_train[\"item_cnt_day\"]\n",
    "    # Aggregate item_id / shop_id item_cnts and revenue at the month level\n",
    "    sales_train_grouped = sales_train.groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).agg(\n",
    "        item_cnt_month=pd.NamedAgg(column=\"item_cnt_day\", aggfunc=\"sum\"),\n",
    "        item_revenue_month=pd.NamedAgg(column=\"item_revenue_day\", aggfunc=\"sum\"),\n",
    "    )\n",
    "\n",
    "    # Merge the grouped data with the index\n",
    "    df = df.merge(\n",
    "        sales_train_grouped,\n",
    "        how=\"left\",\n",
    "        on=[\"date_block_num\", \"shop_id\", \"item_id\"],\n",
    "    )\n",
    "\n",
    "    # Fill emcat_infoy item_cnt entries with 0\n",
    "    df.item_cnt_month = df.item_cnt_month.fillna(0)\n",
    "    df.item_revenue_month = df.item_revenue_month.fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 21.160496,
     "end_time": "2021-05-12T11:10:45.093600",
     "exception": false,
     "start_time": "2021-05-12T11:10:23.933104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = create_testlike_train(train)\n",
    "\n",
    "df = df.merge(items, on=\"item_id\", how=\"left\")\n",
    "df = df.merge(item_categories, on=\"item_category_id\", how=\"left\")\n",
    "df = df.merge(shops, on=\"shop_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show the head of the table to check it looks ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of the target feature  \n",
    "\n",
    "We plot an initial histogram of the target item_cnt_month feature, with a smoothed distribution estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "fg = sns.displot(\n",
    "    df.sample(10000), x=\"item_cnt_month\", bins=100, kde=True\n",
    ")\n",
    "_ = fg.fig.set_size_inches(9, 3)\n",
    "_ = fg.ax.set(title=\"Overall target distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution clearly has a very large peak close to zero.  Creating sales counts for all possible combinations of shop and item each month might lead to lots of entries with zero sales, we should check what proportion of item counts are now zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Proportion of 0-valued targets is {df[df.item_cnt_month==0].shape[0]/df.shape[0]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 85% of target values are zero, which will clearly dominate any distribution plots. To get an idea of the distribution of non-zero values, we plot the distribution again with zero values removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "fg = sns.displot(\n",
    "    df[df.item_cnt_month != 0].sample(10000), x=\"item_cnt_month\", bins=100, kde=True\n",
    ")\n",
    "_ = fg.fig.set_size_inches(9, 3)\n",
    "_ = fg.ax.set(title=\"Distribution of non-zero target values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Proportion of targets greater than 1 is {df[df.item_cnt_month>1].shape[0]/df.shape[0]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with zeros removed, the target is very bottom-heavy, with only around 5% of values above 1, although a small number of items sell much more than this.  \n",
    "\n",
    "The skewness of the distribution makes linear models unsuitable for predicting future sales of items, as assumptions for linear models will not be met. Instead, non-linear models such as decision trees or k-nearest neighbor models would be more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item price distribution\n",
    "We can also plot the distribution of items prices. For this, we take the mean price of the item in months in which it was sold. To do with we create a table which summarizes monthly data across all shops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "def all_shops_frame(df):\n",
    "    items_all_shops = df.groupby([\"date_block_num\", \"item_category_name\", \"item_id\"]).agg(\n",
    "        {\"item_revenue_month\": \"mean\", \"item_cnt_month\": \"sum\"}\n",
    "    )\n",
    "    items_all_shops = items_all_shops.assign(\n",
    "        item_price_mean=lambda x: x.item_revenue_month / x.item_cnt_month\n",
    "    )\n",
    "    return items_all_shops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = all_shops_frame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "fg = sns.displot(df_all, x=\"item_price_mean\", bins=100, kde=True)\n",
    "_ = fg.ax.set_title(\"Item price distribution\")\n",
    "_ = fg.fig.set_size_inches(9,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prices are also highly skewed towards zero. We can display the distribution more clearly by using a log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "fg = sns.displot(df_all, x=\"item_price_mean\", bins=100, kde=True, log_scale=True)\n",
    "_ = fg.ax.set_title(\"Item price distribution (log scale)\")\n",
    "_ = fg.fig.set_size_inches(9,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the price distribution is approximately lognormal with a peak slightly below 10.\n",
    "## Joint distribution of item prices and sales\n",
    "Finally, we can also plot the joint distribution of monthly items sales and mean prices, again using log scales for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "fg = sns.displot(df_all.sample(10000), x=\"item_price_mean\",y=\"item_cnt_month\", bins=25, log_scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reveals no strong overall associations between price and sales, although associations may exist in subgroups of the data.  \n",
    "\n",
    "Outliers are also apparent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.043199,
     "end_time": "2021-05-06T17:36:33.289074",
     "exception": false,
     "start_time": "2021-05-06T17:36:33.245875",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Overall sales trends over time\n",
    "Plotting total sales counts per month shows clear downwards and seasonal trends. Mean sales per item, however, shows a less pronounced downward trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.661776,
     "end_time": "2021-05-06T17:36:34.001433",
     "exception": false,
     "start_time": "2021-05-06T17:36:33.339657",
     "status": "completed"
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(9, 6))\n",
    "sales_resampled = train.groupby(pd.Grouper(key=\"date\", freq=\"m\")).item_cnt_day.sum()\n",
    "_ = sns.lineplot(data=sales_resampled, marker=\".\", markersize=8, ax=axes[0])\n",
    "_ = axes[0].set(xlabel=\"Month\", ylabel=\"item sales\", title=\"Total monthly sales over time, all items\")\n",
    "\n",
    "sales_resampled = df_all.groupby(\"date_block_num\").item_cnt_month.mean()\n",
    "_ = sns.lineplot(data=sales_resampled, marker=\".\", markersize=8, ax=axes[1])\n",
    "_ = axes[1].set(xlabel=\"Date block\", ylabel=\"item sales\", title=\"Mean monthly sales per item over time\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.045351,
     "end_time": "2021-05-06T17:36:34.090927",
     "exception": false,
     "start_time": "2021-05-06T17:36:34.045576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Mean sales per item can also be decomposed into seasonal and continuous trends using the _statsmodels_ package. This show a clear yearly seasonal trend (particularly a peak around the winter holidays) and an overall downwards trend that can be assumed to related to the rise of internet and digital-only sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.755656,
     "end_time": "2021-05-06T17:36:34.889406",
     "exception": false,
     "start_time": "2021-05-06T17:36:34.133750",
     "status": "completed"
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import STL\n",
    "sales_resampled = train.groupby(pd.Grouper(key=\"date\", freq=\"m\")).item_cnt_day.sum()\n",
    "result = STL(sales_resampled).fit()\n",
    "fig = result.plot()\n",
    "fig.set_size_inches((9, 7))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall sales trend is clearly downwards, but there are differences at the item category level. Compare the following trend and seasonal decomposition plots for games for the PS3 and PS4, which show falling and rising trends.  \n",
    "\n",
    "In any case, including a seasonal variable (i.e. a month feature) should help the prediction model capture seasonal trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.755656,
     "end_time": "2021-05-06T17:36:34.889406",
     "exception": false,
     "start_time": "2021-05-06T17:36:34.133750",
     "status": "completed"
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "for catname in [\"Games - PS3\", \"Games - PS4\"]:\n",
    "    sales_resampled = (\n",
    "        train.query(f\"item_category_name=='{catname}'\")\n",
    "        .groupby(pd.Grouper(key=\"date\", freq=\"m\"))\n",
    "        .item_cnt_day.sum()\n",
    "    )\n",
    "    result = STL(sales_resampled).fit()\n",
    "    fig = result.plot()\n",
    "    fig.set_size_inches((9, 7))\n",
    "    plt.suptitle(catname)\n",
    "    fig.tight_layout()\n",
    "del (sales_resampled, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales by item category\n",
    "Each item is assigned one of 80+ categories which identify what kind of product it is and what format it is for. Information about these categories could be predictive because different types of item are likely to sell in different amounts.\n",
    "\n",
    "First we plot mean sales and revenue per item in each category across all shops for the last year of sales data and sort by descending magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "selection = df_all.query(\"date_block_num>21\").drop(columns=\"item_price_mean\")\n",
    "sortidx = selection.groupby(\"item_category_name\")[\"item_cnt_month\"].agg(np.mean).argsort()\n",
    "cat_info = selection.reset_index().melt(id_vars=[\"date_block_num\", \"item_category_name\", \"item_id\"])\n",
    "fg = sns.catplot(\n",
    "    data=cat_info, y=\"item_category_name\", x=\"value\", sharex=False, kind=\"bar\", estimator=np.mean,\n",
    "    order=sortidx[sortidx[::-1]].index, col=\"variable\", col_order=[\"item_cnt_month\", \"item_revenue_month\"]\n",
    ")\n",
    "_ = fg.figure.set_size_inches(12, 18)\n",
    "_ = fg.axes[0][0].set(title=\"Mean monthly sales per item\", xlabel=\"units\", ylabel=None)\n",
    "_ = fg.axes[0][1].set(title=\"Mean monthly revenue per item\", xlabel=\"rubles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most obviously this plot shows that items in one particular category - \"Gifts - bags, albums, mousepads\" - have a much higher sales volume than items in other categories, but comparitively little revenue. This category contains few items and denotes low cost items such as promotional bags and mousemats.  \n",
    "\n",
    "Economic importance may be better represented by plotting summed rather than mean values in each category. Doing so shows that movies and games are the highest-selling categories overall, with PS4 games accounting for the most revenue. As well as being predictive in itself, this information could be useful for deciding what categories to prioritize when building predictive models or allocating promotional resources.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "selection = df_all.query(\"date_block_num>21\").drop(columns=\"item_price_mean\")\n",
    "sortidx = selection.groupby(\"item_category_name\")[\"item_cnt_month\"].agg(np.sum).argsort()\n",
    "cat_info = selection.reset_index().melt(id_vars=[\"date_block_num\", \"item_category_name\", \"item_id\"])\n",
    "fg = sns.catplot(\n",
    "    data=cat_info, y=\"item_category_name\", x=\"value\", sharex=False, kind=\"bar\", estimator=np.sum,\n",
    "    order=sortidx[sortidx[::-1]].index, col=\"variable\", col_order=[\"item_cnt_month\", \"item_revenue_month\"]\n",
    ")\n",
    "_ = fg.figure.set_size_inches(12, 18)\n",
    "_ = fg.axes[0][0].set(title=\"Total monthly sales\", xlabel=\"units\", ylabel=None)\n",
    "_ = fg.axes[0][1].set(title=\"Total monthly revenue\", xlabel=\"rubles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales by shop category\n",
    "Summed sales can similarly be plotted when data is grouped according to shop.\n",
    "\n",
    "As with categories, this shows that a small number of shops, paricularly those in Moscow, sell much more items overall than others. All else being equal, an item sold in these shops can be predicted to sell in greater quantities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "selection = (\n",
    "    df.query(\"date_block_num>21\").sample(10000)\n",
    "    .groupby([\"date_block_num\", \"shop_name\", \"item_id\"])[\n",
    "        [\"item_cnt_month\", \"item_revenue_month\"]\n",
    "    ]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "sortidx = selection.groupby(\"shop_name\")[\"item_cnt_month\"].agg(np.sum).argsort()\n",
    "cat_info = selection.melt(\n",
    "    id_vars=[\"date_block_num\", \"shop_name\", \"item_id\"]\n",
    ")\n",
    "fg = sns.catplot(\n",
    "    data=cat_info,\n",
    "    y=\"shop_name\",\n",
    "    x=\"value\",\n",
    "    sharex=False,\n",
    "    kind=\"bar\",\n",
    "    estimator=np.sum,\n",
    "    order=sortidx[sortidx[::-1]].index,\n",
    "    col=\"variable\",\n",
    "    col_order=[\"item_cnt_month\", \"item_revenue_month\"],\n",
    ")\n",
    "_ = fg.figure.set_size_inches(12, 18)\n",
    "_ = fg.axes[0][0].set(title=\"Total monthly sales\", xlabel=\"units\", ylabel=None)\n",
    "_ = fg.axes[0][1].set(title=\"Total monthly revenue\", xlabel=\"rubles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shop - category sales profile and decomposition of sales\n",
    "Some shops sell more than others, and items in some item categories sell more than others, but are there differences in the relative quantities of items in each category that shops sell?  \n",
    "\n",
    "The individual summed sales per category profile of each shop can be created and is plotted here as a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_info = train.query(\"shop_id!=36 & date_block_num>21\").pivot_table(\n",
    "    values=\"item_cnt_day\",\n",
    "    columns=\"shop_id\",\n",
    "    index=\"item_category_name\",\n",
    "    fill_value=0,\n",
    "    aggfunc=\"sum\",\n",
    ")\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax = sns.heatmap(\n",
    "    cat_info,\n",
    "    cmap=\"mako_r\",\n",
    "    ax=ax,\n",
    "    xticklabels=True,\n",
    "    yticklabels=True,\n",
    "    cbar_kws={\"label\": \"total item sales\"},\n",
    ")\n",
    "# ax = ax.set(ylabel=None, title=\"Shop sales by category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.063403,
     "end_time": "2021-05-06T17:37:00.639721",
     "exception": false,
     "start_time": "2021-05-06T17:37:00.576318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PCA decomposition and clustering of shops  \n",
    "The vertical stripes in the heatmap indicate shops that differ from the mean in some way, but high-dimensional data like this can be difficult to understand without some kind of summary.  \n",
    "\n",
    "Principle component analysis (PCA) lets us decompose high dimensional data into a low dimensional representation that makes it easier to get an overview of patterns in the data. Although the values in the shop-item category sales data do not have the normal distribution assumed by the PCA algorithm, this can be partially corrected by log transforming the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.696637,
     "end_time": "2021-05-06T17:37:03.399324",
     "exception": false,
     "start_time": "2021-05-06T17:37:00.702687",
     "status": "completed"
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_pca(data, n_components, pc1 = 0, pc2 = 1):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    transformed = pca.fit_transform(data)\n",
    "    transformed = pd.DataFrame(transformed)\n",
    "    features = list(range(1, pca.n_components_ + 1))\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    _ = sns.barplot(x=features, y=pca.explained_variance_ratio_ * 100, ax=axes[0,0])\n",
    "    _ = axes[0,0].set(\n",
    "        xlabel=\"PCA features\",\n",
    "        ylabel=\"variance %\",\n",
    "        title=\"Explained variance by principle component\",\n",
    "        xticklabels=features,\n",
    "    )\n",
    "    pcs = [(0, 1), (2, 3), (4, 5)]\n",
    "    for i, pc in enumerate(pcs, start=1):\n",
    "        pc1 = pc[0]\n",
    "        pc2 = pc[1]\n",
    "        x=transformed[pc1]\n",
    "        y=transformed[pc2]\n",
    "        _ = sns.scatterplot(x=x, y=y, ax=axes[i//2, i%2])\n",
    "        _ = axes[i//2, i%2].set(\n",
    "            xlabel=f\"Component {pc1 + 1} score\",\n",
    "            ylabel=f\"Component {pc2 + 1} score\",\n",
    "            title=\"Shop principle component scores\",\n",
    "        )\n",
    "        for j, txt in enumerate(data.index.to_list()):\n",
    "            axes[i//2, i%2].annotate(str(txt), (x[j], y[j]))\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the results of the PCA-decomposition of the shop-item_category sales profiles using the implementation in scikit-learn. The top-left plot shows the proportion of the overall variance explained by each component, while the remaining 3 plots show the shops plotted on scatter plots by their scores on the first 6 principle components.\n",
    "\n",
    "The explained variance plot shows that around 75% of the differences between the shops can be explained by two linear components.\n",
    "\n",
    "Plotting the shops according to their scores on the first two components shows that the shop with shop_id 55 is particularly unlike the other shops, while plotting the shops according to their other components reveals that shop 12 is also an outlier. This makes intuitive sense, as these are internet or virtual shops rather than physical stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.696637,
     "end_time": "2021-05-06T17:37:03.399324",
     "exception": false,
     "start_time": "2021-05-06T17:37:00.702687",
     "status": "completed"
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "start_month=21\n",
    "end_month=32\n",
    "cat_info = df[df.shop_id.isin(test.shop_id.unique())].query(f\"date_block_num>{start_month} & date_block_num<={end_month}\")\n",
    "cat_info = cat_info.pivot_table(values='item_cnt_month', columns='item_category_name', index='shop_id', fill_value=0, aggfunc='sum')\n",
    "pca = plot_pca(cat_info.apply(lambda x: np.log(x + 1)), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insight into what these components correspond to can be gained by plotting the components that map the component scores to the original data. We do this here for the two largest components, and sort the the component mappings for clarity. \n",
    "\n",
    "Looking at component 1, we see that this is mostly highly weighted on \"digital\" categories, i.e. non-physical online downloads, which fits with the internet store 55 having a very high score in this dimension. Further components reveal other dimensions in the differences between shops, such as the sales of music and audiobook products being correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_component(pca, component_num, ax):\n",
    "    sortidx = np.argsort(pca.components_[component_num])[::-1]\n",
    "    _ = sns.barplot(\n",
    "        y=cat_info.columns[sortidx],\n",
    "        x=pca.components_[component_num][sortidx],\n",
    "        orient=\"h\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    _ = ax.set(title=f\"Component {component_num+1}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 12))\n",
    "\n",
    "plot_component(pca, 0, axes[0])\n",
    "plot_component(pca, 1, axes[1])\n",
    "\n",
    "_ = axes[1].set_ylabel(None)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046026,
     "end_time": "2021-05-06T17:36:34.983736",
     "exception": false,
     "start_time": "2021-05-06T17:36:34.937710",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sales as a function of item age\n",
    "\n",
    "The age of items when they are sold can be approximately calculated by subtracting from the sale date the first date or month on which they were sold.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 14.442805,
     "end_time": "2021-05-06T17:36:49.471474",
     "exception": false,
     "start_time": "2021-05-06T17:36:35.028669",
     "status": "completed"
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "def item_shop_age_months(matrix):\n",
    "    matrix[\"item_age\"] = matrix.groupby(\"item_id\")[\"date_block_num\"].transform(\n",
    "    lambda x: x - x.min()\n",
    "    )\n",
    "    matrix[\"new_item\"] = matrix[\"item_age\"] == 0\n",
    "    matrix[\"new_item\"] = matrix[\"new_item\"].astype(\"int8\")\n",
    "    matrix[\"shop_age\"] = (\n",
    "        matrix.groupby(\"shop_id\")[\"date_block_num\"]\n",
    "        .transform(lambda x: x - x.min())\n",
    "        .astype(\"int8\")\n",
    "    )\n",
    "    matrix[\"new_shop\"] = matrix.shop_age == 0\n",
    "    matrix[\"new_shop\"] = matrix[\"new_shop\"].astype(\"int8\")\n",
    "    return matrix\n",
    "\n",
    "df = item_shop_age_months(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046026,
     "end_time": "2021-05-06T17:36:34.983736",
     "exception": false,
     "start_time": "2021-05-06T17:36:34.937710",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Total monthly item sales as a function of item age is plotted below for all items, and separately for items in two representative categories.\n",
    "\n",
    "Plotting total monthly item sales as a function of their ages shows that items tend to sell most when they are new and then decline to a plateau about 1 year later. The slightly lower sales in the first compared to the second month is attributable to items not always being available for the whole first month.  \n",
    "\n",
    "It is also evident that this trend for items to sell most shortly after their release is more evident for some categories, such as movies, compared to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 14.442805,
     "end_time": "2021-05-06T17:36:49.471474",
     "exception": false,
     "start_time": "2021-05-06T17:36:35.028669",
     "status": "completed"
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize = (14,4))\n",
    "_ = sns.lineplot(data=df, x=\"item_age\", y=\"item_cnt_month\", ax=axes[0])\n",
    "_ = axes[0].set(title = \"Monthly item sales by item age, all categories\", xlabel=\"Item age (months)\", ylabel=\"Mean (+95% CI) monthly sales\")\n",
    "_ = sns.lineplot(data=df[df.item_category_name.isin((\"Books - Audiobooks\", \"Cinema - Blu-Ray\"))], x=\"item_age\", y=\"item_cnt_month\",\n",
    "             style=\"item_category_name\", ax=axes[1])\n",
    "_ = axes[1].set(title = \"Monthly item sales by item age, selected categories\", xlabel=\"Item age (months)\", ylabel=\"Mean (+95% CI) monthly sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.080508,
     "end_time": "2021-05-12T11:49:17.915482",
     "exception": false,
     "start_time": "2021-05-12T11:49:17.834974",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Predicting sales from historical sales\n",
    "Even when taking the decline of sales volume over time into account, it seems likely that products that sell well in one month are likely to also sell well in the following month. A column can be created which contains the sales figures from the previous month for the same shop-item combination.  \n",
    "\n",
    "We can create a regression plot of sales counts as a function of previous months sales, for a sample items which are at least a month old. We use log scales on the axes for clarity and plot the estimate of the central tendency (mean) of item_cnt_month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.092228,
     "end_time": "2021-05-12T11:49:18.089717",
     "exception": false,
     "start_time": "2021-05-12T11:49:17.997489",
     "status": "completed"
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "def simple_lag_feature(matrix, lag_feature, lags):\n",
    "    for lag in lags:\n",
    "        newname = lag_feature + f\"_lag_{lag}\"\n",
    "        print(f\"Adding feature {newname}\")\n",
    "        targetseries = matrix.loc[:, [\"date_block_num\", \"item_id\", \"shop_id\"] + [lag_feature]]\n",
    "        targetseries[\"date_block_num\"] += lag\n",
    "        targetseries = targetseries.rename(columns={lag_feature: newname})\n",
    "        matrix = matrix.merge(\n",
    "            targetseries, on=[\"date_block_num\", \"item_id\", \"shop_id\"], how=\"left\"\n",
    "        )\n",
    "        matrix.loc[\n",
    "            (matrix.item_age >= lag) & (matrix.shop_age >= lag) & (matrix[newname].isna()),\n",
    "            newname,\n",
    "        ] = 0\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 110.788322,
     "end_time": "2021-05-12T11:51:08.961233",
     "exception": false,
     "start_time": "2021-05-12T11:49:18.172911",
     "status": "completed"
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "df = simple_lag_feature(df, 'item_cnt_month', lags=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "def regplot(x, y, title):\n",
    "    a = df.query(\"item_age>0 & shop_age>0\").sample(10000)\n",
    "    fg = sns.lmplot(x=x, y=y, data=a)\n",
    "    _ = fg.figure.set_size_inches(7, 5)\n",
    "    _ = fg.ax.set(\n",
    "        xscale=\"log\",\n",
    "        yscale=\"log\",\n",
    "        title=title,\n",
    "        xlabel=f\"{x} (log scale)\",\n",
    "        ylabel=f\"{y} (log scale)\",\n",
    "    )\n",
    "\n",
    "\n",
    "x = \"item_cnt_month_lag_1\"\n",
    "y = \"item_cnt_month\"\n",
    "title = \"Sales counts predicted by the previous month's sales\"\n",
    "regplot(x, y, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.064531,
     "end_time": "2021-05-12T11:14:10.430548",
     "exception": false,
     "start_time": "2021-05-12T11:14:10.366017",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Moving average of previous sales.\n",
    "Single shop-item sales in individual months are mostly low-valued and tend to be noisy. To reduce this noise, windowing methods can be used to calculate a historical mean of sales as a weighted sum of the sales from multiple previous months. An example showing multiple types of window is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.456511,
     "end_time": "2021-05-12T11:14:10.951779",
     "exception": false,
     "start_time": "2021-05-12T11:14:10.495268",
     "status": "completed"
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "shop_id = 16\n",
    "item_id = 482\n",
    "im = df.query(f\"shop_id=={shop_id} & item_id=={item_id}\")[['date_block_num', 'item_cnt_month']]\n",
    "im['moving average'] = im['item_cnt_month'].ewm(halflife=1).mean()\n",
    "im['expanding mean'] = im['item_cnt_month'].expanding().mean()\n",
    "im['rolling 12 month mean'] = im['item_cnt_month'].rolling(window=12, min_periods=1).mean()\n",
    "im = im.set_index('date_block_num')\n",
    "ax = im.plot(figsize=(12,5), marker='.', title='Time series averaging methods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.093464,
     "end_time": "2021-05-12T11:14:11.113944",
     "exception": false,
     "start_time": "2021-05-12T11:14:11.020480",
     "status": "completed"
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "def add_rolling_stats(\n",
    "    df,\n",
    "    features,\n",
    "    window=12,\n",
    "    kind=\"rolling\",\n",
    "    argfeat=\"item_cnt_month\",\n",
    "    aggfunc=\"mean\",\n",
    "    rolling_aggfunc=\"mean\",\n",
    "    dtype=\"float16\",\n",
    "    reshape_source=True,\n",
    "    lag_offset=0,\n",
    "):\n",
    "    def rolling_stat(\n",
    "        df,\n",
    "        source,\n",
    "        feats,\n",
    "        feat_name,\n",
    "        window=12,\n",
    "        argfeat=\"item_cnt_month\",\n",
    "        aggfunc=\"mean\",\n",
    "        dtype=dtype,\n",
    "        lag_offset=0,\n",
    "    ):\n",
    "        # Calculate a statistic on a windowed section of a source table,  grouping on specific features\n",
    "        store = []\n",
    "        for i in range(2 + lag_offset, 35 + lag_offset):\n",
    "            if len(feats) > 0:\n",
    "                mes = (\n",
    "                    source[source.date_block_num.isin(range(max([i - window, 0]), i))]\n",
    "                    .groupby(feats)[argfeat]\n",
    "                    .agg(aggfunc)\n",
    "                    .astype(dtype)\n",
    "                    .rename(feat_name)\n",
    "                    .reset_index()\n",
    "                )\n",
    "            else:\n",
    "                mes = {}\n",
    "                mes[feat_name] = (\n",
    "                    source.loc[\n",
    "                        source.date_block_num.isin(range(max([i - window, 0]), i)), argfeat\n",
    "                    ]\n",
    "                    .agg(aggfunc)\n",
    "                    .astype(dtype)\n",
    "                )\n",
    "                mes = pd.DataFrame(data=mes, index=[i])\n",
    "            mes[\"date_block_num\"] = i - lag_offset\n",
    "            store.append(mes)\n",
    "        store = pd.concat(store)\n",
    "        df = df.merge(store, on=feats + [\"date_block_num\"], how=\"left\")\n",
    "        return df\n",
    "\n",
    "    \"\"\" An issue when using windowed functions is that missing values from months when items recorded no sales are skipped rather than being correctly\n",
    "    treated as zeroes. Creating a pivot_table fills in the zeros.\"\"\"\n",
    "    if (reshape_source == True) or (kind == \"ewm\"):\n",
    "        source = df.pivot_table(\n",
    "            index=features + [\"date_block_num\"],\n",
    "            values=argfeat,\n",
    "            aggfunc=aggfunc,\n",
    "            fill_value=0,\n",
    "            dropna=False,\n",
    "        ).astype(dtype)\n",
    "        for g in features:\n",
    "            firsts = df.groupby(g).date_block_num.min().rename(\"firsts\")\n",
    "            source = source.merge(firsts, left_on=g, right_index=True, how=\"left\")\n",
    "            # Set values before the items first appearance to nan so they are ignored rather than being treated as zero sales.\n",
    "            source.loc[\n",
    "                source.index.get_level_values(\"date_block_num\") < source[\"firsts\"], argfeat\n",
    "            ] = float(\"nan\")\n",
    "            del source[\"firsts\"]\n",
    "        source = source.reset_index()\n",
    "    else:\n",
    "        source = df\n",
    "\n",
    "    if kind == \"rolling\":\n",
    "        feat_name = (\n",
    "            f\"{'_'.join(features)}_{argfeat}_{aggfunc}_rolling_{rolling_aggfunc}_win_{window}\"\n",
    "        )\n",
    "        print(f'Creating feature \"{feat_name}\"')\n",
    "        return rolling_stat(\n",
    "            df,\n",
    "            source,\n",
    "            features,\n",
    "            feat_name,\n",
    "            window=window,\n",
    "            argfeat=argfeat,\n",
    "            aggfunc=rolling_aggfunc,\n",
    "            dtype=dtype,\n",
    "            lag_offset=lag_offset,\n",
    "        )\n",
    "    elif kind == \"expanding\":\n",
    "        feat_name = f\"{'_'.join(features)}_{argfeat}_{aggfunc}_expanding_{rolling_aggfunc}\"\n",
    "        print(f'Creating feature \"{feat_name}\"')\n",
    "        return rolling_stat(\n",
    "            df,\n",
    "            source,\n",
    "            features,\n",
    "            feat_name,\n",
    "            window=100,\n",
    "            argfeat=argfeat,\n",
    "            aggfunc=aggfunc,\n",
    "            dtype=dtype,\n",
    "            lag_offset=lag_offset,\n",
    "        )\n",
    "    elif kind == \"ewm\":\n",
    "        feat_name = f\"{'_'.join(features)}_{argfeat}_{aggfunc}_ewm_hl_{window}\"\n",
    "        print(f'Creating feature \"{feat_name}\"')\n",
    "        source[feat_name] = (\n",
    "            source.groupby(features)[argfeat]\n",
    "            .ewm(halflife=window, min_periods=1)\n",
    "            .agg(rolling_aggfunc)\n",
    "            .to_numpy(dtype=dtype)\n",
    "        )\n",
    "        del source[argfeat]\n",
    "        #         source = source.reset_index()\n",
    "        source[\"date_block_num\"] += 1 - lag_offset\n",
    "        return df.merge(source, on=[\"date_block_num\"] + features, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1887.252376,
     "end_time": "2021-05-12T11:45:38.600357",
     "exception": false,
     "start_time": "2021-05-12T11:14:11.347981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = add_rolling_stats(df, [\"shop_id\", \"item_id\"], kind=\"ewm\", window=1)\n",
    "df = add_rolling_stats(df, [\"shop_id\", \"item_id\"], window=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.075814,
     "end_time": "2021-05-12T11:14:11.277314",
     "exception": false,
     "start_time": "2021-05-12T11:14:11.201500",
     "status": "completed"
    },
    "tags": [
     "remove_input"
    ]
   },
   "source": [
    "We create windowed 12-month average and exponential moving average (in which recent months are weighed more than less recent months) sales count features and display regression plots below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.099385,
     "end_time": "2021-05-12T11:51:09.313575",
     "exception": false,
     "start_time": "2021-05-12T11:51:09.214190",
     "status": "completed"
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "def create_apply_ME(\n",
    "    matrix, grouping_fields, lags=[1], target=\"item_cnt_day_avg\", aggfunc=\"mean\"\n",
    "):\n",
    "    for lag in lags:\n",
    "        newname = \"_\".join(grouping_fields + [target] + [aggfunc] + [f\"lag_{lag}\"])\n",
    "        print(f\"Adding feature {newname}\")\n",
    "        me_series = (\n",
    "            matrix.groupby([\"date_block_num\"] + grouping_fields)[target]\n",
    "            .agg(aggfunc)\n",
    "            .rename(newname)\n",
    "            .reset_index()\n",
    "        )\n",
    "        me_series[\"date_block_num\"] += lag\n",
    "        matrix = matrix.merge(me_series, on=[\"date_block_num\"] + grouping_fields, how=\"left\")\n",
    "        del me_series\n",
    "        matrix[newname] = matrix[newname].fillna(0)\n",
    "        for g in grouping_fields:\n",
    "            firsts = matrix.groupby(g).date_block_num.min().rename(\"firsts\")\n",
    "            matrix = matrix.merge(firsts, left_on=g, right_index=True, how=\"left\")\n",
    "            matrix.loc[\n",
    "                matrix[\"date_block_num\"] < (matrix[\"firsts\"] + (lag)), newname\n",
    "            ] = float(\"nan\")\n",
    "            del matrix[\"firsts\"]\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 110.392034,
     "end_time": "2021-05-12T11:52:59.789458",
     "exception": false,
     "start_time": "2021-05-12T11:51:09.397424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = create_apply_ME(df, [\"item_id\"], target=\"item_cnt_month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "warnings.filterwarnings(\"ignore\", module=\"sklearn\")\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "\n",
    "def regplot(data, x, y, title, ax=None):\n",
    "    score = mutual_info_regression(data[x].to_numpy().reshape(-1, 1), data[y].to_numpy())[0]\n",
    "    score = float(score)\n",
    "    ax = sns.regplot(x=x, y=y, data=data, ax=ax, fit_reg=True, scatter_kws={\"s\": 2})\n",
    "    _ = ax.set(\n",
    "        xscale=\"log\",\n",
    "        yscale=\"log\",\n",
    "        title=title + f\"\\n mutual information = {score:.3}\",\n",
    "        xlabel=f\"{x} (log scale)\",\n",
    "        ylabel=f\"{y} (log scale)\",\n",
    "    )\n",
    "\n",
    "\n",
    "x = \"item_cnt_month_lag_1\"\n",
    "y = \"item_cnt_month\"\n",
    "title = \"Last month\"\n",
    "data = df.query(\"item_age>0 & shop_age>1\").sample(10000)\n",
    "regplot(data, x, y, title, ax=axes[0][0])\n",
    "title = \"Exponential moving average\"\n",
    "regplot(data, \"shop_id_item_id_item_cnt_month_mean_ewm_hl_1\", y, title, axes[0][1])\n",
    "title = \"12 month window\"\n",
    "regplot(data, \"shop_id_item_id_item_cnt_month_mean_rolling_mean_win_12\", y, title, axes[1][0])\n",
    "title = \"Last month all shops\"\n",
    "regplot(data, \"item_id_item_cnt_month_mean_lag_1\", y, title, axes[1][1])\n",
    "_ = plt.suptitle(\"Sales prediction from past sales\")\n",
    "_ = plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The problem of new items\n",
    "Historical sales figures are obviously not directly available for items in their first month of availability, so information about sales of similar items must be used instead. Ideally, multiple measures of similarity should be found and tested.\n",
    "\n",
    "The item_category_id field can be used to group similar items, and the shop_id field can be used to find items sold in the same shop. These can be combined with the item_age engineered feature to calculate mean first-month sales counts for products grouped by item category or shop. This is plotted below for the last available year of sales data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "selection = df.query(\"new_item==True\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,15))\n",
    "sortidx = selection.groupby(\"item_category_name\")[\"item_cnt_month\"].agg(np.mean).argsort()\n",
    "_ = sns.barplot(\n",
    "    data=selection, y=\"item_category_name\", x=\"item_cnt_month\", estimator=np.mean,\n",
    "    order=sortidx[sortidx[::-1]].index, ax=axes[0]\n",
    ")\n",
    "sortidx = selection.groupby(\"shop_name\")[\"item_cnt_month\"].agg(np.mean).argsort()\n",
    "_ = sns.barplot(\n",
    "    data=selection, y=\"shop_name\", x=\"item_cnt_month\", estimator=np.mean,\n",
    "    order=sortidx[sortidx[::-1]].index, ax=axes[1]\n",
    ")\n",
    "_ = axes[0].set(title=\"Mean first month sales by category\", xlabel=\"units\", ylabel=None)\n",
    "_ = axes[1].set(title=\"Mean first month sales by shop\", xlabel=\"rubles\", ylabel=None)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before for items of all ages, the mean new item sales count can also be calculated for each item category - shop combination, plotted below as a heat map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "cat_info = df.query(\"shop_id!=36 & date_block_num>21 & item_age==0\").pivot_table(\n",
    "    values=\"item_cnt_month\",\n",
    "    columns=\"shop_id\",\n",
    "    index=\"item_category_name\",\n",
    "    fill_value=0,\n",
    "    aggfunc=\"mean\",\n",
    ")\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax = sns.heatmap(\n",
    "    cat_info,\n",
    "    cmap=\"mako_r\",\n",
    "    ax=ax,\n",
    "    xticklabels=True,\n",
    "    yticklabels=True,\n",
    "    cbar_kws={\"label\": \"total item sales\"},\n",
    ")\n",
    "ax = ax.set(ylabel=None, title=\"New item sales by category and shop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using text features to find similar items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the information contained in the item categories and shop identities, all items have an associated item_name text feature, which contains a short description of the item that often includes things such as its title, format (e.g. PS4 or PS3) and language. Information can be potentially be extracted from this text string to group similar items.  \n",
    "\n",
    "To aid extraction of information from text it is useful to clean the text of irrelevant special characters and punctuation, excess blank spaces, low-information words such as \"the\", removing diacritics such as accents over letters, and converting all text to lowercase. If necessary this can be performed by regular expression operations, as demonstrated here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "all_stopwords = stopwords.words(\"russian\") + stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "def clean_text(string):\n",
    "    string = re.sub(r\"[^\\w\\s]\", \"\", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    tokens = string.lower().split()\n",
    "    tokens = [t for t in tokens if t not in all_stopwords]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "items[\"item_name_clean\"] = items[\"item_name\"].apply(clean_text)\n",
    "\n",
    "items.loc[1000:, [\"item_name\", \"item_name_clean\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word and n-gram features\n",
    "One way that the item_name field could be used is by extracting individual words or n-grams (groups of sequential words) and treating them as individual binary features. Doing this creates a very large number of features so some kind of filtering is necessary, such as specifying minimum numbers of occurences of a word feature, or setting a threshold on a measure of relevance to the target feature, such as correlation.  \n",
    "\n",
    "Below is an example of the 1 and 2-ngrams producted from a single item_name text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def create_item_id_bow_matrix(items):\n",
    "    all_stopwords = stopwords.words(\"russian\")\n",
    "    all_stopwords = all_stopwords + stopwords.words(\"english\")\n",
    "\n",
    "    vectorizer = CountVectorizer(stop_words=all_stopwords, ngram_range=(1,2))\n",
    "    X = vectorizer.fit_transform(items.loc[:, \"item_name_clean\"])\n",
    "    X = pd.DataFrame.sparse.from_spmatrix(X)\n",
    "    print(f\"{len(vectorizer.vocabulary_)} ngrams found in all items\")\n",
    "    featuremap = {\n",
    "        col: \"ng: \" + token\n",
    "        for col, token in zip(\n",
    "            range(len(vectorizer.vocabulary_)), vectorizer.get_feature_names()\n",
    "        )\n",
    "    }\n",
    "    X = X.rename(columns=featuremap)\n",
    "    X.index = items.index\n",
    "    return X\n",
    "\n",
    "items_short = items.loc[4000:4000,:]\n",
    "X = create_item_id_bow_matrix(items_short) \n",
    "pd.concat((items_short[\"item_name\"], X), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping similar items with similar names\n",
    "Items with different item_ids are often related to each other, such as being different versions of the same video game or movie, and so are likely to have related sales figures. This can be taken advantage of by grouping similar items together based on their item_names.  \n",
    "\n",
    "The Python package TheFuzz implements fuzzy string matching to measure the similarity of sequences of words. This similarity measure can be used to group related items together into a single category. This \"item name group\" feature can then be used as other categorical features.  \n",
    "\n",
    "Below is a short extract of the list of items with their associated name group category designations, showing related items assigned to the same category value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.081646,
     "end_time": "2021-05-12T11:10:47.269648",
     "exception": false,
     "start_time": "2021-05-12T11:10:47.188002",
     "status": "completed"
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def partialmatchgroups(items, sim_thresh):\n",
    "    def strip_brackets(string):\n",
    "        string = re.sub(r\"\\(.*?\\)\", \"\", string)\n",
    "        string = re.sub(r\"\\[.*?\\]\", \"\", string)\n",
    "        return string\n",
    "\n",
    "    items = items.copy()\n",
    "    items[\"nc\"] = items.item_name.apply(strip_brackets)\n",
    "    items[\"ncnext\"] = np.concatenate((items[\"nc\"].to_numpy()[1:], np.array([\"\"])))\n",
    "\n",
    "    def partialcompare(s):\n",
    "        return fuzz.partial_ratio(s[\"nc\"], s[\"ncnext\"])\n",
    "\n",
    "    items[\"partialmatch\"] = items.apply(partialcompare, axis=1)\n",
    "    # Assign groups\n",
    "    grp = 0\n",
    "    for i in range(items.shape[0]):\n",
    "        items.loc[i, \"item_name_group\"] = grp\n",
    "        if items.loc[i, \"partialmatch\"] < sim_thresh:\n",
    "            grp += 1\n",
    "    items = items.drop(columns=[\"nc\", \"ncnext\", \"partialmatch\"])\n",
    "    items['item_name_group'] = items['item_name_group'].apply(int)\n",
    "    return items\n",
    "\n",
    "items = partialmatchgroups(items, 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.081646,
     "end_time": "2021-05-12T11:10:47.269648",
     "exception": false,
     "start_time": "2021-05-12T11:10:47.188002",
     "status": "completed"
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "items.query(\"item_id>3565\")[[\"item_name\", \"item_name_group\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean encoding and none-stationarity of category sales  \n",
    "Categorical features such as name groups and shop ids have useful predictive information, but the relationship between category values and the target variable is not consistent over the training data, as items, item categories and shops increase or (more often) decline in popularity over time, as can be seen when plotting mean sales of music item categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "fg = sns.relplot(\n",
    "    data=df[df[\"item_category_name\"].apply(lambda x: \"music\" in x.lower())],\n",
    "    x=\"date_block_num\",\n",
    "    y=\"item_cnt_month\",\n",
    "    kind=\"line\",\n",
    "    col=\"item_category_name\",\n",
    "    col_wrap=3,\n",
    "    height=2,\n",
    "    aspect=2,\n",
    "    facet_kws={\"sharey\":False},\n",
    ")\n",
    "fg = fg.set_titles(\"Category: {col_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, a model could learn the interactions between individual category values and time periods, but for categorical features with high numbers of values, such as item identities and name groups, this could require very complex models and cause problems with overfitting.  \n",
    "\n",
    "To save the predictive model having to learn the individual relationships between individual categories and specific time periods, a useful solution is to turn categorical features into numerical features by reencoding each value of the category as the mean of the target variable for training items in the same category, for some time window. As with individual items, different time windows can be used.  \n",
    "\n",
    "Below are example windowed mean encodings for 3 values of the item category name feature using 3 different temporal windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1887.252376,
     "end_time": "2021-05-12T11:45:38.600357",
     "exception": false,
     "start_time": "2021-05-12T11:14:11.347981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = add_rolling_stats(df, [\"item_category_name\"], kind=\"ewm\", window=1)\n",
    "df = add_rolling_stats(df, [\"item_category_name\"], window=12)\n",
    "df = add_rolling_stats(df, [\"item_category_name\"], window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"item_category_name_item_cnt_month_mean_ewm_hl_1\",\n",
    "    \"item_category_name_item_cnt_month_mean_rolling_mean_win_12\",\n",
    "    \"item_category_name_item_cnt_month_mean_rolling_mean_win_1\",\n",
    "]\n",
    "data = df.groupby([\"date_block_num\", \"item_category_name\"])[features].mean()\n",
    "newnames = [\"Exponential\", \"12 month square\", \"1 month square\"]\n",
    "data = data.rename(columns={old:new for old, new in zip(features, newnames)})\n",
    "\n",
    "data = data.reset_index().melt(id_vars=[\"date_block_num\", \"item_category_name\"], value_vars=newnames, var_name=\"window\")\n",
    "\n",
    "querycats = [\"Cinema - DVD\", \"Cinema - Blu-Ray\"]\n",
    "\n",
    "fg = sns.relplot(\n",
    "    data=data.loc[data.item_category_name.isin(querycats), :],\n",
    "    x=\"date_block_num\",\n",
    "    y=\"value\",\n",
    "    hue=\"window\",\n",
    "    kind=\"line\",\n",
    "    col=\"item_category_name\",\n",
    "    col_wrap=2,\n",
    "    facet_kws={\"sharey\":False},\n",
    "#     title=\"Windowed mean encodings\"\n",
    ")\n",
    "fg = fg.set_titles(\"Category: {col_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fitting and fitted model exploration\n",
    "The next step after exploratory data analysis should be the creation of a predictive model. Predictive models are obviously useful for planning actions, but a fitted model can also be used for further data exploration by extracting how the model relates features to the final prediction.  \n",
    "\n",
    "## Model explanations with SHAP\n",
    "Here we use the powerful model explanation package SHAP (SHapley Additive exPlanations) to gain additional insights about predictors of item sales. The model explained by SHAP is a LightGBM regressor, a gradient boosted decision tree model.  \n",
    "\n",
    "First a pre-prepared training frame is loaded and used to train the LightGBM regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.10355,
     "end_time": "2021-05-12T11:58:55.409193",
     "exception": false,
     "start_time": "2021-05-12T11:58:55.305643",
     "status": "completed"
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"data/df_analysis.pkl\")\n",
    "\n",
    "# Split train and validation sets from the feature matrix, month 33 used as validation set\n",
    "keep_from_month = 2  # The first couple of months are dropped because of distortions to their features (e.g. wrong item age)\n",
    "test_month = 33\n",
    "dropcols = [\n",
    "    \"shop_id\",\n",
    "    \"item_id\",\n",
    "    #     \"new_item\",\n",
    "]  # The features are dropped to reduce overfitting\n",
    "\n",
    "valid = df.drop(columns=dropcols).loc[df.date_block_num == test_month, :]\n",
    "train = df.drop(columns=dropcols).loc[df.date_block_num < test_month, :]\n",
    "train = train[train.date_block_num >= keep_from_month]\n",
    "X_train = train.drop(columns=\"item_cnt_month\")\n",
    "y_train = train.item_cnt_month\n",
    "X_valid = valid.drop(columns=\"item_cnt_month\")\n",
    "y_valid = valid.item_cnt_month\n",
    "del train, valid, df\n",
    "\n",
    "# These hyperparameters were found by using the hyperparameter optimization framework Optuna to optimize hyperparameters for the validation set.\n",
    "params = {\n",
    "    \"num_leaves\": 966,\n",
    "    \"cat_smooth\": 45.01680827234465,\n",
    "    \"min_child_samples\": 27,\n",
    "    \"min_child_weight\": 0.021144950289224463,\n",
    "    \"max_bin\": 214,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"subsample_for_bin\": 300000,\n",
    "    \"min_data_in_bin\": 7,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"subsample\": 0.6,\n",
    "    \"subsample_freq\": 5,\n",
    "    \"n_estimators\": 8000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1051.630636,
     "end_time": "2021-05-12T12:16:27.313603",
     "exception": false,
     "start_time": "2021-05-12T11:58:55.682967",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"lightgbm\")\n",
    "model = lgbm.LGBMRegressor(**params)\n",
    "\n",
    "categoricals = [\n",
    "    \"item_category_id\",\n",
    "    \"month\",\n",
    "]\n",
    "\n",
    "# Fit the booster using early stopping\n",
    "eval_set = [(X_train, y_train), (X_valid, y_valid)]\n",
    "early_stopping_rounds = 30\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=eval_set,\n",
    "    eval_metric=[\"rmse\"],\n",
    "    verbose=100,\n",
    "    categorical_feature=categoricals,\n",
    "    early_stopping_rounds=early_stopping_rounds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SHAP explainer object is attached to the fitted model and used to generate SHAP values, which quantify the effect of each of the features on each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"shap\")\n",
    "\n",
    "n = 10000  # Takes around an 1 1/2 hours for 10000 rows\n",
    "sample = X_train.sample(n)\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(sample)\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance ranking with beeswarm plots\n",
    "The beeswarm plots below show a summary plot of the effect of each features on each prediction, and ranks the features by overall importance. Each prediction is represented by a point next to each feature name, with the position of the point on the left-right axis indicating the increase or decrease over the baseline prediction caused by the feature value, and the color of the point indicating the value of the feature. For example, a red marker towards the right hand side of the plot indicates that the feature had a high value and caused an increase in the prediction.  \n",
    "\n",
    "Here, the top feature is _shop_id_item_id_item_cnt_month_mean_ewm_hl_1_, which is the exponential moving average of previous sales for the same shop-item combination, with the expected relationship of higher previous sales predicting higher future sales. The fact that the model chose this as the most predictive feature confirms the predictive power of the moving average.  \n",
    "\n",
    "The second most predictive feature, _shop_id_item_category_id_new_item_item_cnt_month_mean_rolling_mean_win_12_ (the name is automatically generated) is a feature that contains the 12-month mean of sales of items with the same category in the same shop, restricted to either new (i.e. first month) items or non-new items depending on whether the item to be predicted is new or not. This is likely to be particularly valuable for predicting sales of new items, a hypothesis which can be explored with further visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values, max_display=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a closer look at feature effects with dependence plots\n",
    "A clearer view of the relationships between the target feature and one or more predictors can be gained with dependence plots.  \n",
    "\n",
    "The following plot shows the relationship between the exponential moving average shop-item sales feature and predictions, showing an approximately linear relationship with increasing variance as feature values increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots(figsize=(7,5))\n",
    "_ = shap.dependence_plot(\n",
    "    \"shop_id_item_id_item_cnt_month_mean_ewm_hl_1\",\n",
    "    shap_values=shap_values.values,\n",
    "    features=shap_values.data,\n",
    "    feature_names=sample.columns.to_list(),\n",
    "    interaction_index=\"new_item\",\n",
    "    ax = ax,\n",
    "    xmax=\"percentile(99.9)\", show=False,\n",
    "    alpha=0.5\n",
    ")\n",
    "_ = ax.set_ylabel(\"SHAP value\")\n",
    "_ = ax.set_xlabel(\"shop-item count moving average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dependence plot can also vizualize interactions between two predictor features by varying point color according to a second variable. Below is plotted the shop-item_category-new_item 12 month windowed mean, with new items coloured red and non-new items coloured blue. We hypothesized that this feature would be particularly valuable for new items, which is confirmed by the steeper slope of the line that can be traced through the red points in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots(figsize=(7,5))\n",
    "shap.dependence_plot(\n",
    "    \"shop_id_item_category_id_new_item_item_cnt_month_mean_rolling_mean_win_12\",\n",
    "    shap_values=shap_values.values,\n",
    "    features=shap_values.data,\n",
    "    feature_names=sample.columns.to_list(),\n",
    "    interaction_index=\"new_item\",\n",
    "    ax=ax,\n",
    "    xmax=\"percentile(99.9)\", show=False,\n",
    "    alpha=0.5\n",
    ")\n",
    "_ = ax.set_ylabel(\"SHAP value\")\n",
    "_ = ax.set_xlabel(\"shop-category-new_item 12 month mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic detection of feature interactions\n",
    "SHAP can also automatically calculate which other feature had the greatest interaction with a given feature when generating predictions. The dependence plot below shows the main effect of the \"first_item_sale_days\" feature, which gives the number of days between the first recorded sale of an item and the first day of the prediction month. This feature shows a main effect of predicting higher sales for newer items, which decreases over time. SHAP identifies as the main interaction feature _shop_cluster_item_id_item_cnt_month_mean_lag_1_, a feature which contains the mean sales of the same item in similar shops, plotting higher values of this feature in red. Looking at the plot, it can be observed that red points tend to be higher than blue points at low values of the main x-axis feature, and lower at higher values of the main feature, indicating that the main trend of decreasing sales with item age is more pronounced for high-selling items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots(figsize=(7,5))\n",
    "shap.dependence_plot(\n",
    "    \"first_item_sale_days\",\n",
    "    shap_values=shap_values.values,\n",
    "    features=shap_values.data,\n",
    "    feature_names=sample.columns.to_list(),\n",
    "    interaction_index=\"auto\",\n",
    "    ax=ax,\n",
    "    xmax=100, show=False,\n",
    "    alpha=0.5\n",
    ")\n",
    "_ = ax.set_ylabel(\"SHAP value\")\n",
    "_ = ax.set_xlabel(\"days since first item sale\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 147.686393,
   "end_time": "2021-05-06T17:38:03.264092",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-06T17:35:35.577699",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
